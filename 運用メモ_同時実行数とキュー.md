# 運用メモ：同時実行数（並列度）・キュー・VRAM（ACE-Step-1.5 REST API）

この資料は、ACE-Step-1.5 REST APIサーバ（`acestep.api_server`）を複数ユーザー／複数アプリから利用する前提で、
「どれくらい同時にジョブを受け付け／実行できるか」を、実装とGPUティア設定に基づいて整理したものです。

## 1. 用語整理（混同しやすいポイント）

- **同時受付（キュー投入）**：APIがジョブを受理して待ち行列に積める数
- **同時実行（並列実行）**：GPU推論が“同時に走る”ジョブ本数
- **batch_size**：1ジョブ内で同時に生成する曲（候補）数（= 1リクエストで何曲出すか）

> 「同時受付」と「同時実行」は別物です。

## 2. 実装上の同時受付（キュー上限）

APIサーバは `asyncio.Queue(maxsize=...)` を持ちます。

- `ACESTEP_QUEUE_MAXSIZE`（デフォルト：`200`）
  - **同時受付（キュー投入）上限**を決めます。
  - 例：多数ユーザーで“受付拒否（キュー満杯）”を避けたい場合は増やします。

注意：
- キューを増やしても、GPUで同時に走る本数は増えません（待ち行列が伸びるだけです）。

## 3. 実装上の同時実行（並列実行）

APIサーバは、キューからジョブを取り出して処理するワーカー（`_queue_worker`）を複数立てられます。

- `ACESTEP_QUEUE_WORKERS`（デフォルト：`1`）
  - **同時実行ジョブ数の上限（キュー消費ワーカー数）**を決めます。

また、重い生成処理はスレッドプールに投げられます。

- `ACESTEP_API_WORKERS`（デフォルト：`1`）
  - スレッドプール（`ThreadPoolExecutor`）の `max_workers` です。

運用のコツ：
- まずは **`ACESTEP_QUEUE_WORKERS` と `ACESTEP_API_WORKERS` を同じ値**に揃えると挙動が読みやすいです。

重要：
- `uvicorn --workers` は、このAPI設計（メモリ内キュー／ジョブストア）上、**1固定推奨**です。
  - その代わりに **`ACESTEP_QUEUE_WORKERS`** を増やして並列度を調整します。

## 4. GPUティア（VRAM）による “目安”

`acestep/gpu_config.py` の設定により、VRAM量からティアが決まり、推奨パラメータの“目安”が用意されています。

### 4.1 24GB（tier6: 16–24GB）

- 最大生成長（LMあり/なし）：480s（8分）
- 目安の最大バッチ（LMあり）：4
- 目安の最大バッチ（LMなし）：8

### 4.2 48GB（unlimited: >=24GB）

- 最大生成長（LMあり/なし）：600s（10分）
- 目安の最大バッチ（LMあり）：8
- 目安の最大バッチ（LMなし）：8

注意：
- これは主に“安全側の目安”です。実際の安定度は、同時実行数、モデル（turbo/base同時ロード有無）、他アプリのVRAM使用量で変動します。

## 5. 今回の条件で絞った推奨（48GB / thinking有効 / 1リクエスト=1曲 / 最大5分=300s）

前提固定：
- リクエスト：`batch_size=1`
- `thinking=true`（LMを使う）
- 曲長：最大300秒

### 5.1 推奨：まずは同時実行 2 から開始

多ユーザー・多アプリ利用を想定する場合、初期値は **同時実行2** が安定しやすいです。

- `ACESTEP_QUEUE_WORKERS=2`
- `ACESTEP_API_WORKERS=2`
- `ACESTEP_QUEUE_MAXSIZE` は状況に応じて（例：`200`→`500`）

### 5.2 余裕があれば 3〜4 に段階的に増やす

他アプリがGPUを使わず、OOMが起きないことを確認しながら段階的に上げます。

- `ACESTEP_QUEUE_WORKERS=3`（または4）
- `ACESTEP_API_WORKERS=3`（または4）

> いきなり大きく上げるより、2→3→4と上げて安定点を探すのが安全です。

## 6. マルチモデル（turbo+base同時ロード）時の注意

`run_api_server_multimodel.sh` のように
- primary: turbo
- secondary: base

を同時ロードすると、単一モデルよりVRAMを使います。

そのため、同時実行数は
- **2から開始**（48GBでもまずは2）

が無難です。

補足：
- `/release_task` の `model` 指定は「同時ロードしているモデルのどれを使うか」の選択です。
- baseをロードしていないサーバに `model=acestep-v15-base` を投げても、turboにフォールバックします。

## 7. 受付を増やしたい vs 実行を増やしたい

- 受付（待ち行列）を増やす：`ACESTEP_QUEUE_MAXSIZE`
  - 例：`ACESTEP_QUEUE_MAXSIZE=500`

- 実行（GPUで同時に動く本数）を増やす：`ACESTEP_QUEUE_WORKERS` と `ACESTEP_API_WORKERS`
  - 例：`ACESTEP_QUEUE_WORKERS=2` / `ACESTEP_API_WORKERS=2`

## 8. 参考：関連する環境変数（抜粋）

キュー／並列度：
- `ACESTEP_QUEUE_MAXSIZE`（受付上限）
- `ACESTEP_QUEUE_WORKERS`（同時実行ジョブ数）
- `ACESTEP_API_WORKERS`（スレッドプールのworker数）

LM（thinking用）：
- `ACESTEP_INIT_LLM`（LM初期化の強制ON/OFF）
- `ACESTEP_LM_MODEL_PATH`（例：`acestep-5Hz-lm-4B`）
- `ACESTEP_LM_BACKEND`（`vllm`/`pt`）

オフロード（低VRAM時の延命）：
- `ACESTEP_OFFLOAD_TO_CPU`
- `ACESTEP_LM_OFFLOAD_TO_CPU`
- `ACESTEP_OFFLOAD_DIT_TO_CPU`

## 9. 実運用の勘所（簡易）

- 多ユーザー用途では「同時実行を上げすぎない」方が、全体の成功率が上がりやすいです。
  - まず 2 で安定させ、必要なら 3〜4 を試す
- `batch_size=1` を維持すると、1リクエストがGPUを占有する時間が短くなりやすく、全体の待ち時間が読みやすいです。
- 生成条件（`inference_steps`、turbo/base）によって処理時間が大きく変わるため、
  “最悪ケース（base + steps多め + 300s）”でも落ちないところを上限として並列度を決めるのが安全です。
